
@incollection{yin_explainable_2022,
	location = {Cham},
	title = {Explainable Artificial Intelligence for Improved Modeling of Processes},
	volume = {13756},
	isbn = {978-3-031-21752-4 978-3-031-21753-1},
	url = {https://link.springer.com/10.1007/978-3-031-21753-1_31},
	pages = {313--325},
	booktitle = {Intelligent Data Engineering and Automated Learning – {IDEAL} 2022},
	publisher = {Springer International Publishing},
	author = {Velioglu, Riza and Göpfert, Jan Philip and Artelt, André and Hammer, Barbara},
	editor = {Yin, Hujun and Camacho, David and Tino, Peter},
	urldate = {2024-05-16},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-031-21753-1_31},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Versión enviada:/home/papabloblo/Zotero/storage/Q2L7UYK9/Velioglu et al. - 2022 - Explainable Artificial Intelligence for Improved M.pdf:application/pdf},
}

@inproceedings{leslie_spectrum_2001,
	location = {Kauai, Hawaii, {USA}},
	title = {The Spectrum Kernel: a String Kernel for {SVM} Protein Classification},
	isbn = {978-981-02-4777-5 978-981-279-962-3},
	url = {http://www.worldscientific.com/doi/abs/10.1142/9789812799623_0053},
	doi = {10.1142/9789812799623_0053},
	shorttitle = {{THE} {SPECTRUM} {KERNEL}},
	eventtitle = {Proceedings of the Pacific Symposium},
	pages = {564--575},
	booktitle = {Biocomputing 2002},
	publisher = {{WORLD} {SCIENTIFIC}},
	author = {Leslie, Christina and Eskin, Eleazar and Noble, William Stafford},
	urldate = {2024-03-20},
	date = {2001-12},
	langid = {english},
}

@article{alt_computing_1995,
	title = {Computing The Fréchet Distance Between Two Polygonal Curves},
	volume = {05},
	issn = {0218-1959, 1793-6357},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218195995000064},
	doi = {10.1142/S0218195995000064},
	abstract = {As a measure for the resemblance of curves in arbitrary dimensions we consider the so-called Fréchet-distance, which is compatible with parametrizations of the curves. For polygonal chains P and Q consisting of p and q edges an algorithm of runtime O(pq log(pq)) measuring the Fréchet-distance between P and Q is developed. Then some important variants are considered, namely the Fréchet-distance for closed curves, the nonmonotone Fréchet-distance and a distance function derived from the Fréchet-distance measuring whether P resembles some part of the curve Q.},
	pages = {75--91},
	number = {1},
	journaltitle = {International Journal of Computational Geometry \& Applications},
	shortjournal = {Int. J. Comput. Geom. Appl.},
	author = {Alt, Helmut and Godau, Michael},
	urldate = {2024-02-28},
	date = {1995-03},
	langid = {english},
	file = {Alt y Godau - 1995 - COMPUTING THE FRÉCHET DISTANCE BETWEEN TWO POLYGON.pdf:/home/papabloblo/Zotero/storage/UTRPYFYS/Alt y Godau - 1995 - COMPUTING THE FRÉCHET DISTANCE BETWEEN TWO POLYGON.pdf:application/pdf},
}

@article{riesen_approximate_2009,
	title = {Approximate graph edit distance computation by means of bipartite graph matching},
	volume = {27},
	issn = {02628856},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S026288560800084X},
	doi = {10.1016/j.imavis.2008.04.004},
	pages = {950--959},
	number = {7},
	journaltitle = {Image and Vision Computing},
	shortjournal = {Image and Vision Computing},
	author = {Riesen, Kaspar and Bunke, Horst},
	urldate = {2024-03-20},
	date = {2009-06},
	langid = {english},
}

@article{fisher_all_2019,
	title = {All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously},
	volume = {20},
	url = {http://jmlr.org/papers/v20/18-760.html},
	pages = {1--81},
	number = {177},
	journaltitle = {Journal of Machine Learning Research},
	author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
	date = {2019},
	file = {Texto completo:/home/papabloblo/Zotero/storage/FI43PIQG/Fisher et al. - 2019 - All Models are Wrong, but Many are Useful Learnin.pdf:application/pdf},
}

@inproceedings{lloyd_statistical_2015,
	title = {Statistical Model Criticism using Kernel Two Sample Tests},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/0fcbc61acd0479dc77e3cccc0f5ffca7-Paper.pdf},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Lloyd, James R and Ghahramani, Zoubin},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	date = {2015},
	file = {Texto completo:/home/papabloblo/Zotero/storage/7G2NSAQ8/Lloyd y Ghahramani - 2015 - Statistical Model Criticism using Kernel Two Sampl.pdf:application/pdf},
}

@inproceedings{kim_examples_2016,
	title = {Examples are not enough, learn to criticize! Criticism for Interpretability},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
	editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
	date = {2016},
	file = {Texto completo:/home/papabloblo/Zotero/storage/2A7YNTTB/Kim et al. - 2016 - Examples are not enough, learn to criticize! Criti.pdf:application/pdf},
}

@article{goldstein_peeking_2015,
	title = {Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation},
	volume = {24},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/full/10.1080/10618600.2014.907095},
	doi = {10.1080/10618600.2014.907095},
	shorttitle = {Peeking Inside the Black Box},
	pages = {44--65},
	number = {1},
	journaltitle = {Journal of Computational and Graphical Statistics},
	shortjournal = {Journal of Computational and Graphical Statistics},
	author = {Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
	urldate = {2024-03-20},
	date = {2015-01-02},
	langid = {english},
	file = {Versión enviada:/home/papabloblo/Zotero/storage/QD2X22SC/Goldstein et al. - 2015 - Peeking Inside the Black Box Visualizing Statisti.pdf:application/pdf},
}

@article{pan_survey_2010,
	title = {A Survey on Transfer Learning},
	volume = {22},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5288526/},
	doi = {10.1109/TKDE.2009.191},
	pages = {1345--1359},
	number = {10},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	urldate = {2024-03-20},
	date = {2010-10},
	file = {Pan y Yang - 2010 - A Survey on Transfer Learning.pdf:/home/papabloblo/Zotero/storage/K7LDFV8V/Pan y Yang - 2010 - A Survey on Transfer Learning.pdf:application/pdf},
}

@article{cignoni_metro_1998,
	title = {\textit{Metro} : Measuring Error on Simplified Surfaces},
	volume = {17},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1467-8659.00236},
	doi = {10.1111/1467-8659.00236},
	shorttitle = {\textit{Metro}},
	abstract = {This paper presents a new tool, Metro, designed to compensate for a deficiency in many simplification methods proposed in literature. Metro allows one to compare the difference between a pair of surfaces (e.g. a triangulated mesh and its simplified representation) by adopting a surface sampling approach. It has been designed as a highly general tool, and it does no assumption on the particular approach used to build the simplified representation. It returns both numerical results (meshes areas and volumes, maximum and mean error, etc.) and visual results, by coloring the input surface according to the approximation error.},
	pages = {167--174},
	number = {2},
	journaltitle = {Computer Graphics Forum},
	shortjournal = {Computer Graphics Forum},
	author = {Cignoni, P. and Rocchini, C. and Scopigno, R.},
	urldate = {2024-03-01},
	date = {1998-06},
	langid = {english},
}

@inproceedings{morain-nicolier_hausdorff_2007,
	location = {Lyon, France},
	title = {Hausdorff Distance based 3D Quantification of Brain Tumor Evolution from {MRI} Images},
	isbn = {978-1-4244-0787-3 978-1-4244-0788-0},
	url = {http://ieeexplore.ieee.org/document/4353615/},
	doi = {10.1109/IEMBS.2007.4353615},
	eventtitle = {2007 29th Annual International Conference of the {IEEE} Engineering in Medicine and Biology Society},
	pages = {5597--5600},
	booktitle = {2007 29th Annual International Conference of the {IEEE} Engineering in Medicine and Biology Society},
	publisher = {{IEEE}},
	author = {Morain-Nicolier, Frederic and Lebonvallet, Stephane and Baudrier, Etienne and Ruan, Su},
	urldate = {2024-03-01},
	date = {2007-08},
	note = {{ISSN}: 1557-170X},
}

@incollection{ceragioli_one_2006,
	location = {Boston},
	title = {One Hundred Years Since the Introduction of the Set Distance by Dimitrie Pompeiu},
	volume = {199},
	isbn = {978-0-387-32774-7},
	url = {http://link.springer.com/10.1007/0-387-33006-2_4},
	pages = {35--39},
	booktitle = {System Modeling and Optimization},
	publisher = {Kluwer Academic Publishers},
	author = {Birsan, T. and Tiba, D.},
	editor = {Ceragioli, F. and Dontchev, A. and Futura, H. and Marti, K. and Pandolfi, L.},
	urldate = {2024-03-01},
	date = {2006},
	langid = {english},
	doi = {10.1007/0-387-33006-2_4},
	note = {Series Title: {IFIP} International Federation for Information Processing},
	file = {Texto completo:/home/papabloblo/Zotero/storage/GTQKXVWY/Birsan y Tiba - 2006 - One Hundred Years Since the Introduction of the Se.pdf:application/pdf},
}

@article{muhlenbein_parallel_1991,
	title = {The parallel genetic algorithm as function optimizer},
	volume = {17},
	issn = {01678191},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167819105800523},
	doi = {10.1016/S0167-8191(05)80052-3},
	pages = {619--632},
	number = {6},
	journaltitle = {Parallel Computing},
	shortjournal = {Parallel Computing},
	author = {Mühlenbein, H. and Schomisch, M. and Born, J.},
	urldate = {2024-02-08},
	date = {1991-09},
	langid = {english},
	file = {Mühlenbein et al. - 1991 - The parallel genetic algorithm as function optimiz.pdf:/home/papabloblo/Zotero/storage/S75EL5CM/Mühlenbein et al. - 1991 - The parallel genetic algorithm as function optimiz.pdf:application/pdf},
}

@article{chaddad_survey_2023,
	title = {Survey of Explainable {AI} Techniques in Healthcare},
	volume = {23},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/23/2/634},
	doi = {10.3390/s23020634},
	abstract = {Artificial intelligence ({AI}) with deep learning models has been widely applied in numerous domains, including medical imaging and healthcare tasks. In the medical field, any judgment or decision is fraught with risk. A doctor will carefully judge whether a patient is sick before forming a reasonable explanation based on the patient’s symptoms and/or an examination. Therefore, to be a viable and accepted tool, {AI} needs to mimic human judgment and interpretation skills. Specifically, explainable {AI} ({XAI}) aims to explain the information behind the black-box model of deep learning that reveals how the decisions are made. This paper provides a survey of the most recent {XAI} techniques used in healthcare and related medical imaging applications. We summarize and categorize the {XAI} types, and highlight the algorithms used to increase interpretability in medical imaging topics. In addition, we focus on the challenging {XAI} problems in medical applications and provide guidelines to develop better interpretations of deep learning models using {XAI} concepts in medical image and text analysis. Furthermore, this survey provides future directions to guide developers and researchers for future prospective investigations on clinical topics, particularly on applications with medical imaging.},
	pages = {634},
	number = {2},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Chaddad, Ahmad and Peng, Jihao and Xu, Jian and Bouridane, Ahmed},
	urldate = {2024-01-30},
	date = {2023-01-05},
	langid = {english},
	file = {Texto completo:/home/papabloblo/Zotero/storage/RJH9P89S/Chaddad et al. - 2023 - Survey of Explainable AI Techniques in Healthcare.pdf:application/pdf},
}

@inbook{chen_explainable_2023,
	location = {Cham},
	title = {Explainable Artificial Intelligence ({XAI}) in Manufacturing},
	isbn = {978-3-031-27960-7 978-3-031-27961-4},
	url = {https://link.springer.com/10.1007/978-3-031-27961-4_1},
	pages = {1--11},
	booktitle = {Explainable Artificial Intelligence ({XAI}) in Manufacturing},
	publisher = {Springer International Publishing},
	author = {Chen, Tin-Chih Toly},
	bookauthor = {Chen, Tin-Chih Toly},
	urldate = {2024-01-30},
	date = {2023},
	langid = {english},
	doi = {10.1007/978-3-031-27961-4_1},
	note = {Series Title: {SpringerBriefs} in Applied Sciences and Technology},
}

@article{zhang_explainable_2022,
	title = {Explainable Artificial Intelligence Applications in Cyber Security: State-of-the-Art in Research},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9875264/},
	doi = {10.1109/ACCESS.2022.3204051},
	shorttitle = {Explainable Artificial Intelligence Applications in Cyber Security},
	pages = {93104--93139},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Zhang, Zhibo and Hamadi, Hussam Al and Damiani, Ernesto and Yeun, Chan Yeob and Taher, Fatma},
	urldate = {2024-01-30},
	date = {2022},
	file = {Texto completo:/home/papabloblo/Zotero/storage/YAEGD2HV/Zhang et al. - 2022 - Explainable Artificial Intelligence Applications i.pdf:application/pdf},
}

@article{tsanas_accurate_2010,
	title = {Accurate Telemonitoring of Parkinson's Disease Progression by Noninvasive Speech Tests},
	volume = {57},
	issn = {0018-9294},
	url = {http://ieeexplore.ieee.org/document/5339170/},
	doi = {10.1109/TBME.2009.2036000},
	pages = {884--893},
	number = {4},
	journaltitle = {{IEEE} Transactions on Biomedical Engineering},
	shortjournal = {{IEEE} Trans. Biomed. Eng.},
	author = {Tsanas, A. and Little, M.A. and {McSharry}, P.E. and Ramig, L.O.},
	urldate = {2024-01-19},
	date = {2010-04},
}

@misc{athanasios_tsanas_parkinsons_2009,
	title = {Parkinsons Telemonitoring},
	url = {https://archive.ics.uci.edu/dataset/189},
	doi = {10.24432/C5ZS3N},
	publisher = {{UCI} Machine Learning Repository},
	author = {Athanasios Tsanas, Max Little},
	urldate = {2024-01-19},
	date = {2009},
}

@article{frechet_sur_1906,
	title = {Sur quelques points du calcul fonctionnel},
	volume = {22},
	issn = {0009-725X, 1973-4409},
	url = {http://link.springer.com/10.1007/BF03018603},
	doi = {10.1007/BF03018603},
	pages = {1--72},
	number = {1},
	journaltitle = {Rendiconti del Circolo Matematico di Palermo},
	shortjournal = {Rend. Circ. Matem. Palermo},
	author = {Fréchet, M. Maurice},
	urldate = {2024-01-15},
	date = {1906-12},
	langid = {italian},
	file = {Versión enviada:/home/papabloblo/Zotero/storage/PTCK3X4E/Fréchet - 1906 - Sur quelques points du calcul fonctionnel.pdf:application/pdf},
}

@article{kullback_information_1951,
	title = {On Information and Sufficiency},
	volume = {22},
	issn = {0003-4851},
	url = {http://projecteuclid.org/euclid.aoms/1177729694},
	doi = {10.1214/aoms/1177729694},
	pages = {79--86},
	number = {1},
	journaltitle = {The Annals of Mathematical Statistics},
	shortjournal = {Ann. Math. Statist.},
	author = {Kullback, S. and Leibler, R. A.},
	urldate = {2024-01-15},
	date = {1951-03},
	langid = {english},
	file = {Texto completo:/home/papabloblo/Zotero/storage/2QITPQ86/Kullback y Leibler - 1951 - On Information and Sufficiency.pdf:application/pdf},
}

@book{manning_introduction_2008,
	edition = {1},
	title = {Introduction to Information Retrieval},
	isbn = {978-0-521-86571-5 978-0-511-80907-1},
	url = {https://www.cambridge.org/core/product/identifier/9780511809071/type/book},
	abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	urldate = {2024-01-15},
	date = {2008-07-07},
	doi = {10.1017/CBO9780511809071},
	file = {Texto completo:/home/papabloblo/Zotero/storage/K4SXT9V3/Manning et al. - 2008 - Introduction to Information Retrieval.pdf:application/pdf},
}

@incollection{seel_measures_2012,
	location = {Boston, {MA}},
	title = {Measures of Similarity},
	isbn = {978-1-4419-1427-9 978-1-4419-1428-6},
	url = {http://link.springer.com/10.1007/978-1-4419-1428-6_503},
	pages = {2147--2150},
	booktitle = {Encyclopedia of the Sciences of Learning},
	publisher = {Springer {US}},
	author = {Ifenthaler, Dirk},
	editor = {Seel, Norbert M.},
	urldate = {2024-01-15},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-1-4419-1428-6_503},
}

@article{friedman_greedy_2001,
	title = {Greedy Function Approximation: A Gradient Boosting Machine},
	volume = {29},
	url = {http://www.jstor.org/stable/2699986},
	pages = {1189--1232},
	number = {5},
	journaltitle = {The Annals of Statistics},
	author = {Friedman, Jerome H.},
	date = {2001},
	langid = {english},
	file = {Friedman - 2001 - Greedy Function Approximation A Gradient Boosting.pdf:/home/papabloblo/Zotero/storage/7JGPLQVW/Friedman - 2001 - Greedy Function Approximation A Gradient Boosting.pdf:application/pdf},
}

@article{friedman_greedy_2001-1,
	title = {Greedy Function Approximation: A Gradient Boosting Machine},
	volume = {29},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/2699986},
	shorttitle = {Greedy Function Approximation},
	abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "{TreeBoost}" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
	pages = {1189--1232},
	number = {5},
	journaltitle = {The Annals of Statistics},
	author = {Friedman, Jerome H.},
	urldate = {2023-12-18},
	date = {2001},
	note = {Publisher: Institute of Mathematical Statistics},
}

@article{barredo_arrieta_explainable_2020,
	title = {Explainable Artificial Intelligence ({XAI}): Concepts, taxonomies, opportunities and challenges toward responsible {AI}},
	volume = {58},
	issn = {15662535},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253519308103},
	doi = {10.1016/j.inffus.2019.12.012},
	shorttitle = {Explainable Artificial Intelligence ({XAI})},
	pages = {82--115},
	journaltitle = {Information Fusion},
	shortjournal = {Information Fusion},
	author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	urldate = {2023-12-05},
	date = {2020-06},
	langid = {english},
	file = {Versión aceptada:/home/papabloblo/Zotero/storage/V72VPTGD/Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf},
}

@article{evgeniou_learning_2005,
	title = {Learning Multiple Tasks with Kernel Methods},
	volume = {6},
	url = {http://jmlr.org/papers/v6/evgeniou05a.html},
	pages = {615--637},
	number = {21},
	journaltitle = {Journal of Machine Learning Research},
	author = {Evgeniou, Theodoros and Micchelli, Charles A. and Pontil, Massimiliano},
	date = {2005},
	file = {Texto completo:/home/papabloblo/Zotero/storage/XBRDLQHK/Evgeniou et al. - 2005 - Learning Multiple Tasks with Kernel Methods.pdf:application/pdf},
}

@inproceedings{parameswaran_large_2010,
	title = {Large Margin Multi-Task Metric Learning},
	volume = {23},
	url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/087408522c31eeb1f982bc0eaf81d35f-Paper.pdf},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Parameswaran, Shibin and Weinberger, Kilian Q},
	editor = {Lafferty, J. and Williams, C. and Shawe-Taylor, J. and Zemel, R. and Culotta, A.},
	date = {2010},
	file = {Texto completo:/home/papabloblo/Zotero/storage/5J6KFFAU/Parameswaran y Weinberger - 2010 - Large Margin Multi-Task Metric Learning.pdf:application/pdf},
}

@inproceedings{carroll_task_2005,
	location = {Montreal, Que., Canada},
	title = {Task similarity measures for transfer in reinforcement learning task libraries},
	volume = {2},
	isbn = {978-0-7803-9048-5},
	url = {http://ieeexplore.ieee.org/document/1555955/},
	doi = {10.1109/IJCNN.2005.1555955},
	abstract = {Recent research in task transfer and task clustering has necessitated the need for task similarity measures in reinforcement learning. Determining task similarity is necessary for selective transfer where only information from relevant tasks and portions of a task are transferred. Which task similarity measure to use is not immediately obvious. It can be shown that no single task similarity measure is uniformly superior. The optimal task similarity measure is dependent upon the task transfer method being employed. We define similarity in terms of tasks, and propose several possible task similarity measures, {dT}, dp, {dQ}, and {dR} which are based on the transfer time, policy overlap, Q-values, and reward structure respectively. We evaluate their performance in three separate experimental situations.},
	eventtitle = {International Joint Conference on Neural Networks 2005},
	pages = {803--808},
	booktitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	publisher = {{IEEE}},
	author = {Carroll, J.L. and Seppi, K.},
	urldate = {2023-12-05},
	date = {2005},
	langid = {english},
	file = {Carroll y Seppi - 2005 - Task similarity measures for transfer in reinforce.pdf:/home/papabloblo/Zotero/storage/TFZTMJM3/Carroll y Seppi - 2005 - Task similarity measures for transfer in reinforce.pdf:application/pdf},
}

@article{breiman_random_2001,
	title = {Random forests},
	volume = {45},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	pages = {5--32},
	number = {1},
	journaltitle = {Machine Learning},
	author = {Breiman, Leo},
	urldate = {2023-11-29},
	date = {2001},
	file = {Texto completo:/home/papabloblo/Zotero/storage/K3X3LQEE/Breiman - 2001 - [No title found].pdf:application/pdf},
}

@article{azzalini_class_1985,
	title = {A Class of Distributions Which Includes the Normal Ones},
	volume = {12},
	issn = {0303-6898},
	url = {https://www.jstor.org/stable/4615982},
	abstract = {A new class of density functions depending on a shape parameter λ is introduced, such that λ=0 corresponds to the standard normal density. The properties of this class of density functions are studied.},
	pages = {171--178},
	number = {2},
	journaltitle = {Scandinavian Journal of Statistics},
	author = {Azzalini, A.},
	urldate = {2023-11-22},
	date = {1985},
	note = {Publisher: [Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
	file = {Azzalini - 1985 - A Class of Distributions Which Includes the Normal.pdf:/home/papabloblo/Zotero/storage/EPGERJMH/Azzalini - 1985 - A Class of Distributions Which Includes the Normal.pdf:application/pdf},
}

@inproceedings{dosilovic_explainable_2018,
	location = {Opatija},
	title = {Explainable artificial intelligence: A survey},
	isbn = {978-953-233-095-3},
	url = {https://ieeexplore.ieee.org/document/8400040/},
	doi = {10.23919/MIPRO.2018.8400040},
	shorttitle = {Explainable artificial intelligence},
	eventtitle = {2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics ({MIPRO})},
	pages = {0210--0215},
	booktitle = {2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics ({MIPRO})},
	publisher = {{IEEE}},
	author = {Dosilovic, Filip Karlo and Brcic, Mario and Hlupic, Nikica},
	urldate = {2023-11-20},
	date = {2018-05},
	file = {Dosilovic et al. - 2018 - Explainable artificial intelligence A survey.pdf:/home/papabloblo/Zotero/storage/7GEW2UIT/Dosilovic et al. - 2018 - Explainable artificial intelligence A survey.pdf:application/pdf},
}

@article{barredo_arrieta_explainable_2020-1,
	title = {Explainable Artificial Intelligence ({XAI}): Concepts, taxonomies, opportunities and challenges toward responsible {AI}},
	volume = {58},
	issn = {15662535},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253519308103},
	doi = {10.1016/j.inffus.2019.12.012},
	shorttitle = {Explainable Artificial Intelligence ({XAI})},
	pages = {82--115},
	journaltitle = {Information Fusion},
	shortjournal = {Information Fusion},
	author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	urldate = {2023-11-20},
	date = {2020-06},
	langid = {english},
	file = {Versión aceptada:/home/papabloblo/Zotero/storage/958PSC99/Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf},
}

@article{eiter_computing_1994,
	title = {Computing Discrete Frechet Distance},
	abstract = {The Fréchet distance between two curves in a metric space is a measure of the similarity between the curves. We present a discrete variation of this measure. It provides good approximations of the continuous measure and can be efficiently computed using a simple algorithm. We also consider variants of discrete Fréchet distance, and find an interesting connection to measuring distance between theories.},
	author = {Eiter, Thomas and Mannila, Heikki},
	date = {1994-05-25},
	file = {Full Text PDF:/home/papabloblo/Zotero/storage/36C2ISPY/Eiter y Mannila - 1994 - Computing Discrete Frechet Distance.pdf:application/pdf},
}

@article{har-peled_frechet_nodate,
	title = {Fréchet distance: How to walk your dog},
	author = {Har-Peled, Sariel},
	langid = {english},
	file = {Har-Peled - Fréchet distance How to walk your dog.pdf:/home/papabloblo/Zotero/storage/CIHMEGBB/Har-Peled - Fréchet distance How to walk your dog.pdf:application/pdf},
}

@inproceedings{lundberg_unified_2017,
	title = {A Unified Approach to Interpreting Model Predictions},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, {SHAP} ({SHapley} Additive {exPlanations}). {SHAP} assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Lundberg, Scott M and Lee, Su-In},
	urldate = {2023-05-15},
	date = {2017},
	file = {Full Text PDF:/home/papabloblo/Zotero/storage/LKFT7YA8/Lundberg y Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf:application/pdf},
}

@article{caruana_multitask_1997,
	title = {Multitask Learning},
	volume = {28},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1007379606734},
	doi = {10.1023/A:1007379606734},
	pages = {41--75},
	number = {1},
	journaltitle = {Machine Learning},
	author = {Caruana, Rich},
	urldate = {2023-05-15},
	date = {1997},
	file = {Texto completo:/home/papabloblo/Zotero/storage/Z3EDKV8C/Caruana - 1997 - [No title found].pdf:application/pdf},
}

@article{apley_visualizing_2020,
	title = {Visualizing the effects of predictor variables in black box supervised learning models},
	volume = {82},
	issn = {1369-7412, 1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/rssb.12377},
	doi = {10.1111/rssb.12377},
	abstract = {In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package {ALEPlot} as supplementary material to implement our proposed method.},
	pages = {1059--1086},
	number = {4},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	shortjournal = {J. R. Stat. Soc. B},
	author = {Apley, Daniel W. and Zhu, Jingyu},
	urldate = {2023-01-04},
	date = {2020-09},
	langid = {english},
	file = {Apley y Zhu - 2020 - Visualizing the effects of predictor variables in .pdf:/home/papabloblo/Zotero/storage/QQMG9GXC/Apley y Zhu - 2020 - Visualizing the effects of predictor variables in .pdf:application/pdf},
}

@article{baxter_model_2000,
	title = {A Model of Inductive Bias Learning},
	volume = {12},
	rights = {Copyright (c)},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/10253},
	doi = {10.1613/jair.731},
	abstract = {A major problem in machine learning is that of inductive    bias: how to choose a learner's hypothesis space so that it is large    enough to contain a solution to the problem being learnt, yet small    enough to ensure reliable generalization from reasonably-sized    training sets.  Typically such bias is supplied by hand through the    skill and insights of experts. In this paper a model for automatically    learning bias is investigated. The central assumption of the model is    that the learner is embedded within an environment of related learning    tasks. Within such an environment the learner can sample from multiple    tasks, and hence it can search for a hypothesis space that contains    good solutions to many of the problems in the environment. Under    certain restrictions on the set of all hypothesis spaces available to    the learner, we show that a hypothesis space that performs well on a    sufficiently large number of training tasks will also perform well    when learning novel tasks in the same environment.  Explicit bounds    are also derived demonstrating that learning multiple tasks within an    environment of related tasks can potentially give much better    generalization than learning a single task.},
	pages = {149--198},
	journaltitle = {Journal of Artificial Intelligence Research},
	author = {Baxter, J.},
	urldate = {2023-02-19},
	date = {2000-03-01},
	langid = {english},
	file = {Full Text PDF:/home/papabloblo/Zotero/storage/6HIZ6FF3/Baxter - 2000 - A Model of Inductive Bias Learning.pdf:application/pdf},
}

@article{zhang_survey_2022,
	title = {A Survey on Multi-Task Learning},
	volume = {34},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/9392366/},
	doi = {10.1109/TKDE.2021.3070203},
	abstract = {Multi-Task Learning ({MTL}) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for {MTL} from the perspective of algorithmic modeling, applications, and theoretical analyses. For algorithmic modeling, we give a deﬁnition of {MTL} and then classify different {MTL} algorithms into ﬁve categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach, and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, {MTL} can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning, and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel, and distributed {MTL} models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use {MTL} to boost their performance and we review representative works. Finally, we present theoretical analyses and discuss several future directions for {MTL}.},
	pages = {5586--5609},
	number = {12},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Zhang, Yu and Yang, Qiang},
	urldate = {2023-01-31},
	date = {2022-12-01},
	langid = {english},
	file = {Zhang y Yang - 2022 - A Survey on Multi-Task Learning.pdf:/home/papabloblo/Zotero/storage/RNAJKY2Z/Zhang y Yang - 2022 - A Survey on Multi-Task Learning.pdf:application/pdf},
}

@inproceedings{ribeiro_why_2016,
	location = {San Francisco California {USA}},
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	isbn = {978-1-4503-4232-2},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	shorttitle = {"Why Should I Trust You?},
	eventtitle = {{KDD} '16: The 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	pages = {1135--1144},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {{ACM}},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2022-12-03},
	date = {2016-08-13},
	langid = {english},
	file = {Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:/home/papabloblo/Zotero/storage/FRTSZC25/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf},
}

@article{vanschoren_meta-learning_2018,
	title = {Meta-Learning: A Survey},
	url = {http://arxiv.org/abs/1810.03548},
	shorttitle = {Meta-Learning},
	abstract = {Meta-learning, or learning to learn, is the science of systematically observing how diﬀerent machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving ﬁeld.},
	journaltitle = {{arXiv}:1810.03548 [cs, stat]},
	author = {Vanschoren, Joaquin},
	urldate = {2022-02-15},
	date = {2018-10-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.03548},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Vanschoren - 2018 - Meta-Learning A Survey.pdf:/home/papabloblo/Zotero/storage/244N329T/Vanschoren - 2018 - Meta-Learning A Survey.pdf:application/pdf},
}

@misc{ruder_overview_2017,
	title = {An Overview of Multi-Task Learning in Deep Neural Networks},
	url = {http://arxiv.org/abs/1706.05098},
	doi = {10.48550/arXiv.1706.05098},
	abstract = {Multi-task learning ({MTL}) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of {MTL}, particularly in deep neural networks. It introduces the two most common methods for {MTL} in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help {ML} practitioners apply {MTL} by shedding light on how {MTL} works and providing guidelines for choosing appropriate auxiliary tasks.},
	number = {{arXiv}:1706.05098},
	publisher = {{arXiv}},
	author = {Ruder, Sebastian},
	urldate = {2025-01-18},
	date = {2017-06-15},
	eprinttype = {arxiv},
	eprint = {1706.05098 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/papabloblo/Zotero/storage/YVC8SWPM/Ruder - 2017 - An Overview of Multi-Task Learning in Deep Neural .pdf:application/pdf;Snapshot:/home/papabloblo/Zotero/storage/YQUYKCGZ/1706.html:text/html},
}

@inproceedings{ma_modeling_2018,
	location = {London United Kingdom},
	title = {Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts},
	isbn = {978-1-4503-5552-0},
	url = {https://dl.acm.org/doi/10.1145/3219819.3220007},
	doi = {10.1145/3219819.3220007},
	eventtitle = {{KDD} '18: The 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	pages = {1930--1939},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Ma, Jiaqi and Zhao, Zhe and Yi, Xinyang and Chen, Jilin and Hong, Lichan and Chi, Ed H.},
	urldate = {2025-01-18},
	date = {2018-07-19},
	langid = {english},
	file = {Texto completo:/home/papabloblo/Zotero/storage/HGBU9HA4/Ma et al. - 2018 - Modeling Task Relationships in Multi-task Learning.pdf:application/pdf},
}

@misc{yu_unleashing_2024,
	title = {Unleashing the Power of Multi-Task Learning: A Comprehensive Survey Spanning Traditional, Deep, and Pretrained Foundation Model Eras},
	url = {http://arxiv.org/abs/2404.18961},
	doi = {10.48550/arXiv.2404.18961},
	shorttitle = {Unleashing the Power of Multi-Task Learning},
	abstract = {{MTL} is a learning paradigm that effectively leverages both task-specific and shared information to address multiple related tasks simultaneously. In contrast to {STL}, {MTL} offers a suite of benefits that enhance both the training process and the inference efficiency. {MTL}'s key advantages encompass streamlined model architecture, performance enhancement, and cross-domain generalizability. Over the past twenty years, {MTL} has become widely recognized as a flexible and effective approach in various fields, including {CV}, {NLP}, recommendation systems, disease prognosis and diagnosis, and robotics. This survey provides a comprehensive overview of the evolution of {MTL}, encompassing the technical aspects of cutting-edge methods from traditional approaches to deep learning and the latest trend of pretrained foundation models. Our survey methodically categorizes {MTL} techniques into five key areas: regularization, relationship learning, feature propagation, optimization, and pre-training. This categorization not only chronologically outlines the development of {MTL} but also dives into various specialized strategies within each category. Furthermore, the survey reveals how the {MTL} evolves from handling a fixed set of tasks to embracing a more flexible approach free from task or modality constraints. It explores the concepts of task-promptable and -agnostic training, along with the capacity for {ZSL}, which unleashes the untapped potential of this historically coveted learning paradigm. Overall, we hope this survey provides the research community with a comprehensive overview of the advancements in {MTL} from its inception in 1997 to the present in 2023. We address present challenges and look ahead to future possibilities, shedding light on the opportunities and potential avenues for {MTL} research in a broad manner. This project is publicly available at https://github.com/junfish/Awesome-Multitask-Learning.},
	number = {{arXiv}:2404.18961},
	publisher = {{arXiv}},
	author = {Yu, Jun and Dai, Yutong and Liu, Xiaokang and Huang, Jin and Shen, Yishan and Zhang, Ke and Zhou, Rong and Adhikarla, Eashan and Ye, Wenxuan and Liu, Yixin and Kong, Zhaoming and Zhang, Kai and Yin, Yilong and Namboodiri, Vinod and Davison, Brian D. and Moore, Jason H. and Chen, Yong},
	urldate = {2025-01-18},
	date = {2024-04-29},
	eprinttype = {arxiv},
	eprint = {2404.18961 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}
